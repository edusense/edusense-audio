Features
========

Current version of EduSense provides the following list of features. As we use GraphQL schema for our storage server APIs, we explain the features around the schema in the following sections. For the comprehensive, up-to-date schema, please take a look at [here](/storage/query/schema.go).

## Video Features

### Metadata

Each processed video frame has four fields: `frameNumber`, `timestamp`, `thumbnail` and `people`. 

```
type VideoFrame {
  frameNumber: Int!
  timestamp: Time!
  thumbnail: Thumbnail
  people: [Person]
}
```

* `frameNumber`: positive integer frame number
* `timestamp`: time when the frame is processed
* `thumbnail`: base64-encoded JPEG of the raw image (240x135 by default)
* `people`: processed results for each person

### Person

Each person entry contains three raw keypoints generated by OpenPose (`body`, `face` and `hand`), integer unique per-frame identifier (`openposeId`) and collection of EduSense inference results (`inference`).

```
type Person {
  body: [Int!]
  face: [Int!]
  hand: [Int!]
  openposeId: Int
  inference: VideoInference
}
```

* `body`: body keypoints. Keypoints are stored in one-dimentional array. Each keypoint has three values: integer x-coordinate, integer y-coordinate and binarized integer confidence number (0 or 1). These values are flattened in the array, resulting in an array of size *n* having *n/3* unique keypoints. For example, the first few elements of the array contain \[<x-coord of first keypoint>, <y-coord of first keypoint>, <confidence of first keypoint>, <x-coord of second keypoint>, ....\]. More details about each keypoint and value can be found at [OpenPose repository](https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/output.md).
* `face`: face keypoints. Keypoints are represented in the same way as body keypoints.
* `hand`: hand keypoints. Keypoints are represented in the same way as body keypoints.
* `openposeId`: An integer number given by OpenPose library. This is ordered left-to-right; the left-most person gets id 0 and the right-most person gets the highest number for his/her id.
* `inference`: inference results in `VideoInference` object.

### Video Inferences

`VideoInference` has multiple inference result fields: `posture` for posture-level inferences, `face` for face-level inferences and `head` for head-level inferences. Also, it has `trackingId` field which is a unique integer identifier for inter-frame tracking.

```
type VideoInference {
  posture: Posture
  face: Face
  head: Head
  trackingId: Int
}
```

#### Inferred Posture Features

`Posture` contains multiple inference results: `armPose`, `sitStand`, `centroidDelta` and `armDelta`.

```
enum ArmPose {
  handsRaised
  armsCrossed
  handsOnFace
  other
  error
}

enum SitStand {
  sit
  stand
  error
}

type Posture {
  armPose: ArmPose
  sitStand: SitStand
  centroidDelta: [Int!]
  armDelta: [[Int!]!]
}
```

* `armPose`: Arm pose inference result. This can be one of `handsRaised`, `armsCrossed`, `handsOnFace`, `other` and `error`. Error normally means we did not get all the arm keypoints that are required.
* `sitStand`: sit/stand inference result. This can be one of `sit`, `stand` and `error`.
* `centroidDelta`: an integer array of size 2 that notes x-coordinate and y-coordinate delta of this body from last processed frame. This is used to estimate the level of movement.
* `armDelta`: an array of x-coordinate and y-coordinate delta pairs of arm keypoints.

#### Inferred Face Features

`Face` has multiple features: `boundingBox`, `mouthOpen`, `mouthSmile` and `orientation`.

```
enum MouthOpen {
  open
  closed
  error
}

enum MouthSmile {
  smile
  noSmile
  error
}

enum FaceOrientation {
  front
  back
  error
}

type Face {
  boundingBox: [[Int!]!]
  mouthOpen: MouthOpen
  mouthSmile: MouthSmile
  orientation: FaceOrientation
}
```

* `boundingBox`: x-coordinate and y-coordinate pairs that represent two diagonal corners of the face bounding box.
* `mouthOpen`: whether mouth is `open` or `closed`. It can have `error` value if we do not have enough keypoints to make inference.
* `mouthSmile`: whether the person is smiling(`smile`) or not(`noSmile`). This can also be `error` if we have insufficent number of mouth keypoints.
* `orientation`: whether this person is facing the camera (`front`) or turning back from the camera(`back`).

#### Inferred Head Features

Any inferred result with head is included in `Head` struct. All of the fields are related to head orientation.

```
  type Head {
  roll: Float!
  pitch: Float!
  yaw: Float!
  translationVector: [Float!]
  gazeVector: [[Int!]!]
}
```

* `roll`: float roll of head orientation
* `pitch`: float pitch of head orientation
* `yaw`: float yaw of head orientation
* `translationVector`: float array representing translation vector of head orientation
* `gazeVector`: 2-dimensional array of two \[x-coordinate, y-coordinate\] pairs that note gaze direction. The coordinates are respect to  video frames.

## Audio Features

### Metadata

Like video frames, each processed audio frame is indexed by `frameNumber` and `timestamp`. Frame numbers in video and audio frames are numbered differently; the frame number in this audio frame data does not match with the one in video frame.

```

enum Channel {
  instructor
  student
}

type AudioFrame {
  frameNumber: Int!
  timestamp: Time!
  channel: Channel!
  audio: Audio!
}
```

* `frameNumber`: positive integer frame number, does not match with the numbers in video frames.
* `timestamp`: time when this frame has processed
* `channel`: whether this audio frame data is for student or instructor side microphone.
* `audio`: Container for audio features.

### Acoustic Features

In `Audio` container, we have two raw acoustic features(i.e., `amplitude` and `melFrequency`) and one container for inferred features.

```
type Audio {
  amplitude: Float!,
  melFrequency: [[Float!]!]!
  inference: AudioInference!
}
```

* `amplitude`: a positive float number between 0.0 and 1.0 that represents amplitude of the ambient sound
* `melFrequency`: two dimensional float array that represents featurized mel frequency
* `inference`: Collection of audio features.

#### Inferred Audio Features

`AudioInference` struct contains inferred `speech` feature. For speech, we provide confidence value for speech inference and who is the speaker (student, instructor or ambient).

```
enum Speaker {
  ambient
  student
  instructor
}

type Speech {
  confidence: Float!
  speaker: Speaker!
}

type AudioInference {
  speech: Speech!
}
```

* `confidence`: float confidence value between 0.0 and 1.0. Higher value means it is more likely that a person is speaking.
* `speaker`: speaker contains how is the speaker in right now: `student`, `instructor` or `ambient`.
